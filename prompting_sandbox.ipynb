{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d35e01",
   "metadata": {},
   "source": [
    "# üß™ Prompting Sandbox\n",
    "Compare how different LLMs (Gemini, GPT-4, Hugging Face Transformers) respond to the same prompt.\n",
    "\n",
    "- Designed for **in-session demos**\n",
    "- Fully modular and editable\n",
    "- Logs outputs for structured evaluation\n",
    "\n",
    "**Models included**:\n",
    "- OpenAI (gpt-4, gpt-3.5)\n",
    "- Gemini Pro (via Google AI Studio)\n",
    "- Hugging Face Transformers (e.g., Mistral, Zephyr, LLaMA, etc.)\n",
    "\n",
    "üîí *This notebook does not store or send keys‚Äîuse environment variables or secret manager.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Setup Cell ‚Äî Load APIs and secrets\n",
    "import os\n",
    "\n",
    "# Example: Use environment variables (recommended)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")  # placeholder\n",
    "HF_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# You can also use a .env file via dotenv\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967eff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Define Prompt\n",
    "task_description = \"\"\"Summarize the main health risks from the policy text below in 3 bullet points.\n",
    "Use plain language suitable for a community health workshop.\"\"\"\n",
    "\n",
    "context = \"\"\"The national report on rural health identifies increased risk of untreated diabetes, mental health isolation, and reduced access to vaccination.\"\"\"\n",
    "\n",
    "full_prompt = f\"{task_description}\\n\\n{context}\"\n",
    "print(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ GPT-4 / GPT-3.5 Completion\n",
    "import openai\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": full_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"GPT-4 Output:\")\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ó Hugging Face Model via Transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\", token=HF_API_TOKEN)\n",
    "\n",
    "output = summarizer(full_prompt, max_new_tokens=200)[0]['generated_text']\n",
    "print(\"Hugging Face Output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52966969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÆ Gemini Pro via Google AI Studio ‚Äî Placeholder\n",
    "# This will depend on your preferred client interface (e.g., langchain or direct API)\n",
    "# Example:\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# model = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GEMINI_API_KEY)\n",
    "\n",
    "print(\"Gemini API integration requires manual setup.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb1ac6",
   "metadata": {},
   "source": [
    "## üìä Compare Outputs\n",
    "Copy-paste responses here or use your own structured evaluation rubric.\n",
    "\n",
    "Use this to compare:\n",
    "- Clarity\n",
    "- Relevance\n",
    "- Format compliance\n",
    "- Reasoning quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b32cc4",
   "metadata": {},
   "source": [
    "## üìè Evaluation Rubric\n",
    "\n",
    "Use this rubric to manually assess each model's output on a 1‚Äì5 scale.\n",
    "\n",
    "| Criterion         | Description                                 | Scale |\n",
    "|------------------|---------------------------------------------|-------|\n",
    "| Relevance        | Does the output address the task directly?  | 1‚Äì5   |\n",
    "| Clarity          | Is the response easy to understand?         | 1‚Äì5   |\n",
    "| Format Accuracy  | Does the output match the required format?  | 1‚Äì5   |\n",
    "| Reasoning Logic  | Are examples or structure logical/coherent? | 1‚Äì5   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90987251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Basic Manual Scoring Framework\n",
    "def rubric_score(output, reference=None):\n",
    "    print(\"Rate this output on the rubric (1-5):\")\n",
    "    relevance = int(input(\"Relevance: \"))\n",
    "    clarity = int(input(\"Clarity: \"))\n",
    "    format_accuracy = int(input(\"Format Accuracy: \"))\n",
    "    reasoning = int(input(\"Reasoning Logic: \"))\n",
    "    return {\n",
    "        \"relevance\": relevance,\n",
    "        \"clarity\": clarity,\n",
    "        \"format\": format_accuracy,\n",
    "        \"reasoning\": reasoning\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9db511",
   "metadata": {},
   "source": [
    "## üéõ Temperature & Token Exploration\n",
    "\n",
    "Test the same prompt under different settings to explore behavior.\n",
    "\n",
    "**Suggested Ranges:**\n",
    "- `temperature`: 0.3 (precise), 0.7 (balanced), 1.0 (creative)\n",
    "- `max_tokens`: 100, 250, 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Run GPT-4 with different temperature values\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n--- Temperature: {temp} ---\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ],\n",
    "        temperature=temp,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e4b73",
   "metadata": {},
   "source": [
    "## üß≠ Additional Features (not implemented here)\n",
    "\n",
    "- ‚úÖ Add retry decorators using `tenacity` for production-grade API stability\n",
    "- ‚úÖ Fallback models if main model fails\n",
    "- ‚úÖ `ipywidgets` interface for dropdown selection of prompts or model versions\n",
    "- ‚úÖ Visual charts with matplotlib/plotly for output length, scoring trends\n",
    "- ‚úÖ Cost estimator comparing GPT-4 vs 3.5 vs local models\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
