{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf96ee4",
   "metadata": {},
   "source": [
    "# üß± Colab Foundations: Setup, Navigation & Best Practices\n",
    "\n",
    "Welcome to **Colab Foundations** ‚Äî your starting point for working effectively with Google Colab notebooks in applied research and AI workflows.\n",
    "\n",
    "Google Colab (short for *Collaboratory*) provides a cloud-based Python environment with no local setup required. It's especially useful for:\n",
    "- Running code directly from GitHub or Google Drive\n",
    "- Prototyping AI and data science workflows\n",
    "- Teaching, collaboration, and reproducible research\n",
    "\n",
    "This notebook walks you through the foundational setup required for professional-grade usage:\n",
    "- ‚úÖ Enabling autocomplete and tooltips\n",
    "- üìÅ Managing files and directories\n",
    "- üîë Securely configuring API keys\n",
    "- üß™ Running and debugging code cells\n",
    "- üîÑ Integrating with GitHub and version control\n",
    "\n",
    "By the end of this notebook, you'll have a fully configured, clean Colab environment ready to interface with AI APIs, data pipelines, and LLM-driven tools.\n",
    "\n",
    "> ‚ö†Ô∏è All steps are beginner-friendly but structured for reproducible and professional usage in research and applied AI settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f7051",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 1: Enable Autocomplete & Tooltips in Colab\n",
    "Colab offers built-in tools that help beginners write better code by predicting what you're typing and explaining functions as you use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32e122",
   "metadata": {},
   "source": [
    "### üí¨ What is Autocomplete?\n",
    "Autocomplete suggests completions for what you're typing ‚Äî like variable names, methods, or Python keywords. It reduces typing errors and helps you remember the correct structure of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344e572",
   "metadata": {},
   "source": [
    "### üí° What Are Tooltips?\n",
    "Tooltips show a small pop-up with the function's signature and docstring ‚Äî telling you:\n",
    "- What the function does\n",
    "- What inputs (parameters) it takes\n",
    "- What output it returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ab548",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è How To Enable It in Colab\n",
    "1. Click `Tools > Settings`\n",
    "2. Select the **Editor** tab\n",
    "3. Enable:\n",
    "   - ‚úÖ Show code completions\n",
    "   - ‚úÖ Show function call tooltips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973eca4",
   "metadata": {},
   "source": [
    "üîé **Why this matters**: Later, when you're working with libraries like `transformers` or `openai`, autocomplete and tooltips help you explore their capabilities without memorizing every function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382896fa",
   "metadata": {},
   "source": [
    "## üîê Step 2: Install Gemini & OpenAI Libraries ‚Äî API Powered LLM Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab473beb",
   "metadata": {},
   "source": [
    "LLMs are accessed through APIs ‚Äî you send a prompt to a remote server, and it returns a generated response.\n",
    "\n",
    "To work with these models, you‚Äôll install the official SDKs (software development kits) using `pip`. These SDKs simplify access and let you format your requests directly from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231a1a1",
   "metadata": {},
   "source": [
    "These libraries don‚Äôt do any inference locally. Instead, they:\n",
    "1. Authenticate using your API key\n",
    "2. Send data to Google or OpenAI‚Äôs servers\n",
    "3. Return model output in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb210ae",
   "metadata": {},
   "source": [
    "### üîí API Keys & Colab Secret Manager\n",
    "To keep your key secure:\n",
    "```python\n",
    "from google.colab import secret\n",
    "API_KEY = secret.get(\"OPENAI_API_KEY\")\n",
    "```\n",
    "You‚Äôll set the key manually using:\n",
    "```python\n",
    "secret.set(\"OPENAI_API_KEY\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c8e84",
   "metadata": {},
   "source": [
    "## üß† Step 3: Load, Inspect, and Manipulate Data ‚Äî LLM Prep Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4417f0",
   "metadata": {},
   "source": [
    "You'll often work with text stored in spreadsheets or text files. First, load this data into Python for inspection and cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8bdd1c",
   "metadata": {},
   "source": [
    "### üì• Option 1: Load CSV File from Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Read uploaded CSV\n",
    "df = pd.read_csv(\"your_file.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd60b9d6",
   "metadata": {},
   "source": [
    "### üìÇ Option 2: Load from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Replace with your path\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/LLM_Course/sample.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d24ea",
   "metadata": {},
   "source": [
    "### üìÑ Option 3: Load a Text File Line-by-Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample_text.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[:5]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf28938",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and remove whitespace\n",
    "df[\"clean\"] = df[\"text\"].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows (e.g. health-related text)\n",
    "df_health = df[df[\"category\"] == \"health\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a column into two new ones\n",
    "df[[\"title\", \"body\"]] = df[\"combined\"].str.split(\"|\", expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e274c0",
   "metadata": {},
   "source": [
    "üìå **Why it matters**: Before feeding data to an LLM (like via Hugging Face), it must be clean, structured, and usually in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Lowercasing a text column\n",
    "df[\"text_clean\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# Filtering based on conditions\n",
    "filtered = df[df[\"category\"] == \"health\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fcd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking structured columns\n",
    "df[[\"col1\", \"col2\"]] = df[\"combined\"].str.split(\"|\", expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e10e4e",
   "metadata": {},
   "source": [
    "## üß± Step 4: Functions, Parameters, and Lambda Expressions\n",
    "Functions allow you to structure and reuse code. You can define your own, or use functions from libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620503b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    if \"good\" in text:\n",
    "        return \"positive\"\n",
    "    return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f46666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with parameter + return\n",
    "result = classify(\"This is good\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b66649",
   "metadata": {},
   "source": [
    "### üîç How to Know What Parameters a Function Has:\n",
    "- Use autocomplete: type the function name and press `Tab`\n",
    "- Use `help()`:\n",
    "```python\n",
    "from transformers import pipeline\n",
    "help(pipeline)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9319a",
   "metadata": {},
   "source": [
    "## üì¶ Step 5: What Are These Libraries and Why Do We Use Them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d5afb",
   "metadata": {},
   "source": [
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| `transformers` | Load and use LLMs (Hugging Face) |\n",
    "| `openai` | Connect to OpenAI‚Äôs GPT models |\n",
    "| `google-generativeai` | Access Google Gemini models |\n",
    "| `chromadb` | Store and search documents locally (used in RAG) |\n",
    "| `pandas` | Load and clean tabular data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd840857",
   "metadata": {},
   "source": [
    "## ü§ñ Step 6: Preparing for Hugging Face Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09751198",
   "metadata": {},
   "source": [
    "### LLM Output Format ‚Äî Dictionary Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6642f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\"label\": \"POSITIVE\", \"score\": 0.98}\n",
    "print(response[\"label\"], response[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2b5c3",
   "metadata": {},
   "source": [
    "### Passing Batch Inputs to a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3464f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This is great\", \"Not so good\"]\n",
    "for text in texts:\n",
    "    print(\"Input:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a075427",
   "metadata": {},
   "source": [
    "### Creating a Function for LLM Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b35607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_classify(text):\n",
    "    text = text.lower().strip()\n",
    "    # placeholder for pipeline call\n",
    "    return {\"label\": \"POSITIVE\", \"input\": text}\n",
    "\n",
    "clean_and_classify(\"  THIS IS GOOD  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963900e",
   "metadata": {},
   "source": [
    "üîó Next Step: Use Hugging Face to Run Real LLM Tasks\n",
    "In the next session, you will:\n",
    "\n",
    "Load models from Hugging Face using the pipeline interface\n",
    "\n",
    "Run tasks like sentiment analysis, summarization, and text classification\n",
    "\n",
    "Work with outputs in dictionary format ‚Äî unpack, print, analyze\n",
    "\n",
    "Build reusable functions for LLM-powered workflows\n",
    "\n",
    "\n",
    "üìò All of this will be covered in the next notebook: [python_minimalist.md](python_minimalist.md)\n",
    "\n",
    "You'll be writing real prompts, getting real results, and laying the groundwork for more advanced logic in Day 2 and Day 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b01368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
