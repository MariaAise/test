{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e263c7",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Setup Instructions\n",
    "This notebook uses the following tools and APIs for document retrieval and LLM-based generation:\n",
    "\n",
    "## üîß Libraries Required:\n",
    "- `langchain`\n",
    "- `chromadb`\n",
    "- `pypdf` (for PDF parsing)\n",
    "- `sentence-transformers` (embedding model)\n",
    "- `google-generativeai` (for Gemini API)\n",
    "- `openai` (for OpenAI GPT models)\n",
    "\n",
    "Install them using the following cells where necessary.\n",
    "\n",
    "## üîê API Keys Required:\n",
    "- **Google Gemini API key** (set to `os.environ['GOOGLE_API_KEY']`)\n",
    "- **OpenAI API key** (set to `openai.api_key`)\n",
    "\n",
    "You can obtain keys from:\n",
    "- Google Gemini: https://makersuite.google.com/app/apikey\n",
    "- OpenAI: https://platform.openai.com/account/api-keys\n",
    "\n",
    "Make sure to paste your keys in the designated code cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91b8a4",
   "metadata": {},
   "source": [
    "# üìò Day 3 RAG Colab Notebook\n",
    "This notebook demonstrates a complete Retrieval-Augmented Generation (RAG) workflow:\n",
    "- Load PDFs from a folder\n",
    "- Chunk and embed with SentenceTransformers\n",
    "- Store in ChromaDB\n",
    "- Run semantic search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üü© Install dependencies\n",
    "!pip install -q langchain chromadb pypdf sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üü® Load PDFs from folder\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_dir = Path(\"./pdfs\")  # <-- Replace with your folder path\n",
    "all_docs = []\n",
    "\n",
    "for pdf_path in pdf_dir.glob(\"*.pdf\"):\n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    docs = loader.load()\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} pages from {len(list(pdf_dir.glob('*.pdf')))} PDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üüß Split documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(all_docs)\n",
    "print(f\"Generated {len(chunks)} text chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üü• Embed chunks and store in ChromaDB\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db\")\n",
    "db.persist()\n",
    "print(\"Embeddings saved to ChromaDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b410d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Run semantic search\n",
    "query = \"What are the findings about youth crime prevention?\"\n",
    "results = db.similarity_search(query, k=3)\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"--- Result {i+1} ---\\n{res.page_content[:500]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09eb540",
   "metadata": {},
   "source": [
    "## üß† Generate Answer with Gemini\n",
    "Use Gemini model to answer a query based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e318b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Install Google Generative AI SDK if not already installed\n",
    "!pip install -q google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6600e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîë Setup Gemini API key\n",
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] = \"your-api-key-here\"  # Replace with your actual key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì° Generate response from Gemini using retrieved content\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "retrieved_text = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "prompt = f\"Answer the following question based on the retrieved content:\\n\\n{retrieved_text}\\n\\nQuestion: {query}\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befdacf5",
   "metadata": {},
   "source": [
    "## üìé Gemini Output: Without vs With Citations\n",
    "This section demonstrates how to:\n",
    "- Generate a plain Gemini response (no source info)\n",
    "- Generate a response **with inline source references** using metadata from the retrieved chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ûñ Gemini Response WITHOUT Source Info\n",
    "plain_prompt = f\"Answer the question based on the following content:\\n\\n{retrieved_text}\\n\\nQuestion: {query}\"\n",
    "response_plain = model.generate_content(plain_prompt)\n",
    "print(response_plain.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca52e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ûï Gemini Response WITH Source References\n",
    "# Append [source_n] markers using doc.metadata\n",
    "retrieved_with_refs = []\n",
    "for i, doc in enumerate(results):\n",
    "    marker = f\"[source_{i+1}]\"\n",
    "    content_with_marker = doc.page_content.strip() + f\"\\n\\n{marker}\"\n",
    "    retrieved_with_refs.append(content_with_marker)\n",
    "\n",
    "referenced_text = \"\\n\\n\".join(retrieved_with_refs)\n",
    "citation_prompt = f\"Answer the question using the content below. Include [source_n] in your answer to show where facts came from.\\n\\n{referenced_text}\\n\\nQuestion: {query}\"\n",
    "response_cited = model.generate_content(citation_prompt)\n",
    "print(response_cited.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88efa4",
   "metadata": {},
   "source": [
    "## ü§ñ OpenAI Output: With vs Without Citations\n",
    "Now let's demonstrate the same generation using OpenAI's GPT model instead of Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üü¶ Install OpenAI SDK if not installed\n",
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8780308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîë Setup OpenAI API key\n",
    "import openai\n",
    "openai.api_key = \"your-openai-api-key-here\"  # Replace with your OpenAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa21960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ûñ OpenAI Response WITHOUT Source Info\n",
    "openai_prompt = f\"Answer the question based on the following content:\\n\\n{retrieved_text}\\n\\nQuestion: {query}\"\n",
    "completion_plain = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": openai_prompt}]\n",
    ")\n",
    "print(completion_plain['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056546ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ûï OpenAI Response WITH Source References\n",
    "referenced_text_openai = referenced_text  # reusing same `[source_n]` markers\n",
    "openai_prompt_cited = f\"Answer the question using the content below. Include [source_n] in your answer to show where facts came from.\\n\\n{referenced_text_openai}\\n\\nQuestion: {query}\"\n",
    "completion_cited = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": openai_prompt_cited}]\n",
    ")\n",
    "print(completion_cited['choices'][0]['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
